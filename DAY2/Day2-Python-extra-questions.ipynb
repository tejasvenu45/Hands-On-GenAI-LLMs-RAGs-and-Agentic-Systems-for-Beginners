{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python- Extra Questions\n",
        "This notebook has extra questions based on python fundamentals- data structures, conditions, loops and functions- but in the domain of text processing. Please do it if you are interested- no compulsion. This is not evaluated.\n",
        "* Solutions are given in cells following question cells. You can refer to it if you cannot solve it after attempting."
      ],
      "metadata": {
        "id": "sD4k7f4TANmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1"
      ],
      "metadata": {
        "id": "2W8fTH97i9fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extension of the probability of words program:\n",
        "* input a string of words from the user\n",
        "* find the probability of occurrence of a word given its previous word- store this in a dictionary **{(previous_word, word): probability of occurrence}**. Note that you need to consider the number of pairs of consecutive words, so the denominator to find probability is (number of words - 1).\n",
        "* input a word from the user (you can assume the word appears in the above string of words). Consider this to be the **previous_word**\n",
        "* print the **word** with the highest probability, considering the above **previous_word**\n",
        "\n",
        "Example:\n",
        "* Enter a string:\n",
        "  * the book is on the table on a carpet on the floor\n",
        "* Input string: the book is on the table on a carpet on the floor\n",
        "  * Number of words in the string: 12\n",
        "* Word: Word Probability dictionary:\n",
        "    * {('the', 'book'): 0.09090909090909091, ('book', 'is'): 0.09090909090909091, ('is', 'on'): 0.09090909090909091, ('on', 'the'): 0.18181818181818182, ('the', 'table'): 0.09090909090909091, ('table', 'on'): 0.09090909090909091, ('on', 'a'): 0.09090909090909091, ('a', 'carpet'): 0.09090909090909091, ('carpet', 'on'): 0.09090909090909091, ('the', 'floor'): 0.09090909090909091}\n",
        "* Enter a word from the above string to predict its next token:\n",
        "  * on\n",
        "* Word with the highest probability is:\n",
        "  * the"
      ],
      "metadata": {
        "id": "t7jhzCY6jbGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code here\n"
      ],
      "metadata": {
        "id": "3IwPi5vM7mVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Solution:"
      ],
      "metadata": {
        "id": "mToHjOFyjAff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a string as input\n",
        "print(\"Enter a string:\")\n",
        "input_string = input()\n",
        "\n",
        "words_in_input_string = input_string.split()\n",
        "word_count = len(words_in_input_string)\n",
        "\n",
        "# Find frequency of occurrence of each word in the input string\n",
        "word_dict = {}\n",
        "\n",
        "for i in range(1, len(words_in_input_string)):\n",
        "  previous_word = words_in_input_string[i-1]\n",
        "  current_word = words_in_input_string[i]\n",
        "  key = (previous_word, current_word)\n",
        "\n",
        "  if key not in word_dict:\n",
        "    word_dict[key] = 0\n",
        "  word_dict[key] += 1\n",
        "\n",
        "# Find probability of occurrence of each word in the input string\n",
        "for key in word_dict:\n",
        "  word_dict[key] /= (word_count-1) # number of pairs of words in all, is 1 subracted from number of words in the string\n",
        "\n",
        "print(\"\\nInput string:\", input_string)\n",
        "print(\"Number of words in the string:\", word_count)\n",
        "print(\"\\nWord: Word Probability dictionary:\\n\", word_dict)\n",
        "\n",
        "# Predicting\n",
        "print(\"Enter a word from the above string to predict its next token:\")\n",
        "given_token_to_predict = input()\n",
        "# assuming a word from the initial input string is entered\n",
        "highest_prob = 0\n",
        "word_with_highest_prob = \"\"\n",
        "for key, value in word_dict.items():\n",
        "  previous_word = key[0]\n",
        "  word = key[1]\n",
        "  if previous_word == given_token_to_predict:\n",
        "    if value > highest_prob:\n",
        "      highest_prob = value\n",
        "      word_with_highest_prob = word\n",
        "print(f\"Word with the highest probability is: {word_with_highest_prob}\")"
      ],
      "metadata": {
        "id": "OZQ-iLG9iNS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15b255c-01c5-4f35-9f23-4418f0b9676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a string:\n",
            "the book is on the table on a carpet on the floor\n",
            "\n",
            "Input string: the book is on the table on a carpet on the floor\n",
            "Number of words in the string: 12\n",
            "\n",
            "Word: Word Probability dictionary:\n",
            " {('the', 'book'): 0.09090909090909091, ('book', 'is'): 0.09090909090909091, ('is', 'on'): 0.09090909090909091, ('on', 'the'): 0.18181818181818182, ('the', 'table'): 0.09090909090909091, ('table', 'on'): 0.09090909090909091, ('on', 'a'): 0.09090909090909091, ('a', 'carpet'): 0.09090909090909091, ('carpet', 'on'): 0.09090909090909091, ('the', 'floor'): 0.09090909090909091}\n",
            "Enter a word from the above string to predict its next token:\n",
            "on\n",
            "Word with the highest probability is: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats, you built your own text predictor model through the above exercise! This is called bi-gram analysis (considering two consecutive tokens at a time).\n",
        "\n",
        "Note/ See also:\n",
        "* You can extend this further by considering preprocessing text (see below)- to consider 'The' and 'the' as the same word, remove special characters and punctuation, etc. Essentially standardizing text.\n"
      ],
      "metadata": {
        "id": "6W_afHo1-pYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2"
      ],
      "metadata": {
        "id": "8IYYTSF7aD0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you write a function to pre-process text:\n",
        "*  remove punctuation (commas, full stops), special characters (&, *, $, %, #, @, !)- use for example **string = string.replace('&', ' ')** for each of the punctuations and special characters\n",
        "*  make all alphabets lowercase- use **string = string.lower()**\n",
        "*  tokenization- splitting the string of words into words (using **list_of_strings = string.split()**)\n",
        "*  remove stopwords (stopwords are words that do not add much to the meaning of the text, and appear very frequently). Use **stopwords = (\"a\", \"the\", \"is\", \"am\", \"and\", \"but\", \"i\")**. Remove a word from the word list **list_of_strings** if it appears in the **stopwords** tuple.\n",
        "\n",
        "Example:\n",
        "* function should take a string parameter: \"I like apples and oranges, but apples more!\"\n",
        "* function should return a list of tokens (can have duplicate tokens)"
      ],
      "metadata": {
        "id": "5_cN-oaGaHtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = (\"a\", \"the\", \"is\", \"am\", \"and\", \"but\", \"i\")\n",
        "\n",
        "# code here"
      ],
      "metadata": {
        "id": "C5VshyDCAYq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution:"
      ],
      "metadata": {
        "id": "gnNCZN5yga_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = (\"a\", \"the\", \"is\", \"am\", \"and\", \"but\", \"i\")\n",
        "def preprocess_text(string):\n",
        "  \"\"\"\n",
        "  parameters: a string\n",
        "\n",
        "  processing:\n",
        "    remove punctuation, special characters\n",
        "    make all alphabets lowercase\n",
        "    tokenization\n",
        "    remove stopwords\n",
        "\n",
        "  returns: a list of strings\n",
        "  \"\"\"\n",
        "\n",
        "  # remove punctuation, special characters- using regex makes this more elegant\n",
        "  string = string.replace(\",\", \"\")\n",
        "  string = string.replace(\".\", \"\")\n",
        "  string = string.replace(\"!\", \"\")\n",
        "  string = string.replace(\"&\", \"\")\n",
        "  string = string.replace(\"*\", \"\")\n",
        "  string = string.replace(\"'\", \"\")\n",
        "  string = string.replace(\"\\\"\", \"\")\n",
        "  string = string.replace(\"#\", \"\")\n",
        "\n",
        "  # make all alphabets lowercase\n",
        "  string = string.lower()\n",
        "\n",
        "  # tokenization\n",
        "  tokens = string.split()\n",
        "\n",
        "  # remove stopwords\n",
        "  tokens = [x for x in tokens if x not in STOPWORDS]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "preprocess_text(\"I like apples and oranges, but apples more!\")"
      ],
      "metadata": {
        "id": "_unsHr3T0TzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c737b6f-cedd-4d0f-cd1a-2469fff39428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['like', 'apples', 'oranges', 'apples', 'more']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats, you built your own text preprocessing function through the above exercise! You can extend this by adding more functionalities (more stopwords, testing on a bigger corpus of text, using lesser lines of code- in replace characters, etc., also assigning a number to each unique word in the corpus, etc.)\n",
        "* NLTK (Natural Language Toolkit) is a library for natural language processing, used heavily in the area of LLMs and oher language models, with built-in functions for the same. You built some basic implementations of functions in NLTK by yourself!"
      ],
      "metadata": {
        "id": "lFM13Bae_I55"
      }
    }
  ]
}